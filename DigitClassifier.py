# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/176axmvFMIGhBvQuDziGRKgsLkVZ0Irt7
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Hyperparameters
batch_size = 8
learning_rate = 0.001
epochs = 10
momentum = 0.9

#function to preprocess the data
def preprocess_data(lines):
    data = [list(map(float, line.split())) for line in lines]
    labels = torch.tensor([int(d[0]) for d in data], dtype=torch.long)
    pixels = torch.tensor([d[1:] for d in data], dtype=torch.float32)

    # Reshape to 16x16 and rotate the image
    pixels = torch.rot90(pixels.view(-1, 16, 16), k=3, dims=(1, 2))

    # Reshape and normalize the input data
    pixels = pixels.view(-1, 1, 16, 16) / 255.0

    return TensorDataset(pixels, labels)

# Load data
with open('zip_train.txt', 'r') as train_file:
    train_lines = train_file.readlines()

train_dataset = preprocess_data(train_lines)

# Create DataLoader for training set
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
# Training loop
def training(model):
    print("\nTraining:")
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(momentum, 0.999))
    for epoch in range(epochs):
        model.train()
        for inputs, labels in train_loader:
            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')
    return model
import torch.nn as nn
import torch.nn.init as init

class FullyConnectedClassifier(nn.Module):
    def __init__(self):
        super(FullyConnectedClassifier, self).__init__()

        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(16 * 16, 128)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)

        self.fc2 = nn.Linear(128, 64)
        self.tanh = nn.Tanh()

        self.fc3 = nn.Linear(64, 32)
        self.sigmoid = nn.Sigmoid()

        self.fc4 = nn.Linear(32, 10)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)

        x = self.dropout(x)
        x = self.fc2(x)
        x = self.tanh(x)

        x = self.dropout(x)
        x = self.fc3(x)
        x = self.sigmoid(x)

        x = self.dropout(x)
        x = self.fc4(x)
        x = self.softmax(x)
        return x

print('\nTraining Fully connected Network',learning_rate)
FCModel = training (FullyConnectedClassifier())

import torch.nn.functional as F

class LocallyConnectedLayer(nn.Module):

    def __init__(self, in_channels, out_channels, input_height, input_width, kernel_size):
        super(LocallyConnectedLayer, self).__init__()
        self.weights = nn.Parameter(torch.rand(out_channels, in_channels, kernel_size, kernel_size))
        self.bias = nn.Parameter(torch.zeros(out_channels))
        self.input_height = input_height
        self.input_width = input_width
        self.kernel_size = kernel_size
        self.out_channels = out_channels
        self.in_channels = in_channels

    def forward(self, x):
        batch_size, in_channels = x.size()
        x = x.view(batch_size, 1, self.input_height, self.input_width)
        output_height = self.input_height - self.kernel_size + 1
        output_width = self.input_width - self.kernel_size + 1

        x_unf = F.unfold(x, self.kernel_size, padding=0)
        weights_unf = self.weights.view(self.weights.size(0), -1)

        out_unf = torch.matmul(weights_unf, x_unf) + self.bias.view(-1, 1)
        out = out_unf.view(batch_size, -1, output_height, output_width)

        return out

    def parameters(self):
        return [self.weights, self.bias]

class LocallyConnectedClassifier(nn.Module):
    def __init__(self):
        super(LocallyConnectedClassifier, self).__init__()

        self.flatten = nn.Flatten()
        self.layer1 = LocallyConnectedLayer(1, 128, 16, 16, 3)
        self.relu = nn.ReLU()

        self.layer2 = nn.Linear(128 * 14 * 14, 10)
        self.tanh = nn.Tanh()

    def forward(self, x):
        x = self.flatten(x)
        x = self.layer1(x)
        x = self.relu(x)

        x = x.view(x.size(0), -1)
        x = self.layer2(x)
        x = self.tanh(x)

        return x

    def parameters(self):
        return list(self.layer1.parameters()) + list(self.layer2.parameters())

print('\nTraining Locally connected Network')
LCModel = training (LocallyConnectedClassifier())

class ConvolutionalClassifier(nn.Module):
    def __init__(self):
        super(ConvolutionalClassifier, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(32 * 8 * 8, 128)
        self.sigmoid = nn.Sigmoid()

        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, 10)
        self.softmax = nn.Softmax(dim=1)

    def initialize_weights_uniform(self):
        init.uniform_(self.conv1.weight)
        init.uniform_(self.conv1.bias)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.flatten(x)
        x = self.fc1(x)
        x = self.sigmoid(x)

        x = self.dropout(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

print('\nTraining Convolutional Neural : ',learning_rate)
model = ConvolutionalClassifier();
model.initialize_weights_uniform();
CNNModel = training(model)
import itertools

# Load test data
with open('zip_test.txt', 'r') as test_file:
    test_lines = test_file.readlines()
# Preprocess the test data
test_dataset = preprocess_data(test_lines)
# Create DataLoader for test set
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

def evaluate_ensemble(models):
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad():
        for test_inputs, test_labels in test_loader:
            ensemble_predictions = torch.zeros(len(test_labels), dtype=torch.float)  # Initialize as float
            modelNames = [];
            for model in models:
                modelNames.append(type(model).__name__)
                model.eval()
                model_predictions = model(test_inputs)
                ensemble_predictions += torch.argmax(model_predictions, dim=1).float()  # Convert to float

            # Ensure division result is of type Long
            ensemble_predictions = (ensemble_predictions / len(models)).long()

            correct_predictions += torch.sum(ensemble_predictions == test_labels)
            total_samples += len(test_labels)

    # Calculate accuracy
    accuracy = correct_predictions.double() / total_samples
    print("\nEnsemble Testing of models:", str(modelNames))
    print("Number of Correct Predictions (Test):", correct_predictions.item())
    print("Total Test Samples:", total_samples)
    print("Test Accuracy:", accuracy.item())
    print("Test Error Rate:",1 - accuracy.item())

def ensemble(models):
    # Get all combinations of three models
    combinations = itertools.product(models, repeat=3)
    for ensemble_combination in combinations:
        evaluate_ensemble(ensemble_combination)
models = [FCModel, LCModel, CNNModel]
ensemble(models)